{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS4WFwwfnyfd"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from skimage import io \n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfyO0T1Zo8iC",
        "outputId": "9a434f54-93a1-475d-9c77-3a6431beb04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx1-RiRs11_H",
        "outputId": "9d8d9904-dda5-4fdd-9e73-820707d8099c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "280\n",
            "torch.Size([3, 256, 256]) torch.Size([3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "#total images in dataset = 1540\n",
        "#for each image 35 divisions made, and in total 44 images, so 35*44 = 1540 \n",
        "#after excluding 1.0.0 , 2.0.0 , 3.0.0 , 4.0.0 we will have 140 images less\n",
        "#so in total  = 1400 images\n",
        "#80% of 1400 = 1120\n",
        "\n",
        "class MedicalImage(Dataset):\n",
        "  \n",
        "  def __init__(self, data_path):\n",
        "    self.path = data_path\n",
        "    all_files = os.listdir(data_path)\n",
        "    l = len(all_files)\n",
        "    self.store10x = [] # stores 35*8 = 280, 10x images\n",
        "\n",
        "    for item in all_files:\n",
        "\n",
        "      if (item.split(\"_\")[1]=='1' or item.split(\"_\")[1]=='2') and item.split(\"_\")[2]=='0': \n",
        "        self.store10x.append(item) # 280 (10x images) are filtered\n",
        "      else:\n",
        "          pass\n",
        "          \n",
        "    print(len(self.store10x))\n",
        "\n",
        "  def __len__(self):\n",
        "    x = len(self.store10x)\n",
        "    return (x)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img_path = f'{self.path}{self.store10x[index]}'\n",
        "    img = Image.open(img_path)\n",
        "    z = self.store10x[index].split('_')[2]\n",
        "    if (z==0):\n",
        "      sf = 2.5\n",
        "    else:\n",
        "      sf = 10.0\n",
        "    img = img.resize((256,256),resample=Image.BILINEAR)\n",
        "    w,h = img.size\n",
        "\n",
        "    blur_img = img.resize((int(w/sf),int(h/sf)),resample=Image.BILINEAR)\n",
        "    blur_img = blur_img.resize((w,h),resample=Image.BILINEAR)\n",
        "    \n",
        "    input_img = tf.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])(tf.ToTensor()(blur_img))\n",
        "    target_img = tf.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])(tf.ToTensor()(img))\n",
        "    return(input_img, target_img)\n",
        "\n",
        "data = MedicalImage('/content/drive/MyDrive/Medical Image Dataset/')  # PRETAM'S MACHINE\n",
        "#data = MedicalImage('/content/drive/MyDrive/Cropped Medical Images/')  # SWARNENDU'S MACHINE\n",
        "inp,tar = data[24]\n",
        "print(inp.shape, tar.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdbHcrnH16d_"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 2e-4 \n",
        "BATCH_SIZE = 2\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS_IMG = 3\n",
        "IN_CHANNNELS = 3\n",
        "OUT_CHANNELS = 8\n",
        "NUM_EPOCHS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5pAvnqH19hu"
      },
      "outputs": [],
      "source": [
        "#Load Data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "trainloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "testloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aLPgmbT2B8Y",
        "outputId": "3a449972-f0fb-4db3-9828-7bb08303eaac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "class HierConv(Module):    # HIERARCHICAL CONVOLUTIONS\n",
        "    def __init__(self,IC,OC,level=5,remap_K=1,remap_S=1,remap_P=0):\n",
        "        super(HierConv, self).__init__()\n",
        "        self.level = level\n",
        "        assert(self.level>=5 and self.level%2==1), \"level should be a odd number greater than or equal to 5\"\n",
        "        self.layers = {}\n",
        "        for l in range(5,self.level+1,2):\n",
        "            self.layers[str(l)]=nn.ModuleList([nn.Conv2d(IC,OC,k,1,k//2).cuda() for k in range(3,l+1,2)])\n",
        "        self.output_dim=0\n",
        "        for i in range(5, self.level+1, 2):\n",
        "            for j in range (1, i-1, 2):\n",
        "                self.output_dim += j**2 \n",
        "        self.remap = nn.Conv2d(OC*self.output_dim,OC,remap_K,remap_S,remap_P,1,OC)\n",
        "    def forward(self, x):\n",
        "        outs={}\n",
        "        for l in range(5,self.level+1,2):\n",
        "            outs[str(l)]=[self.layers[str(l)][i](x) for i in range(l//2)]\n",
        "        B,C,H,W = outs['5'][0].shape\n",
        "        final_out=torch.zeros(B,C*self.output_dim,H,W).cuda()\n",
        "        for b in range(B):\n",
        "            for c in range(C):\n",
        "                stackedFeatures = []\n",
        "                for l in range(5,self.level+1,2):\n",
        "                    d=-1\n",
        "                    stackedFeatures.append(outs[str(l)][d][b,c,:,:])\n",
        "                    d-=1\n",
        "                    for p in range(3,l,2):\n",
        "                        for sh in range(0,p):                                                                                \n",
        "                            for sw in range(0,p):\n",
        "                                stackedFeatures.append(F.pad(outs[str(l)][d][b,c,:,:],(sh,p-1-sh,sw,p-1-sw))[p//2:-(p//2),p//2:-(p//2)]) \n",
        "                        d-=1\n",
        "                m = self.output_dim \n",
        "                final_out[b,m*c:m*c+m,:,:]=torch.stack(stackedFeatures, 0)\n",
        "        output = self.remap(final_out)\n",
        "        return(output)\n",
        "\n",
        "hc = HierConv(3,8,5).cuda()\n",
        "input = torch.randn(4,3,256,256).cuda() #b,c,h,w\n",
        "output = hc(input)\n",
        "output.shape "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhoQWAHV9q4b",
        "outputId": "e722792e-b57d-41b9-b093-075228c4c9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionGen(nn.Module):\n",
        "  def __init__(self, in_dim):\n",
        "    super(SelfAttentionGen, self).__init__()\n",
        "\n",
        "    self.query_conv = nn.Conv2d(in_dim, in_dim//8, kernel_size=1)\n",
        "    self.key_conv = nn.Conv2d(in_dim, in_dim//8, kernel_size=1)\n",
        "    self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
        "\n",
        "    self.gamma = nn.Parameter(torch.zeros(1))\n",
        "    \n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, height, width = x.size()\n",
        "    print(11)\n",
        "    proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "    print(12)\n",
        "    proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
        "    print(13)\n",
        "    energy = torch.bmm(proj_query, proj_key)\n",
        "    print(14)\n",
        "    attention = self.softmax(energy)\n",
        "    print(15)\n",
        "    proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
        "    print(16)\n",
        "    out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "    print(17)\n",
        "    out = out.view(batch_size, channels, height, width)\n",
        "    print(18)\n",
        "    out = self.gamma * out + x\n",
        "    print(19)\n",
        "    return out"
      ],
      "metadata": {
        "id": "1vX707gDRVlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBO78IVB2Eko"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self,IC,OC,K,S,P):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.layers = nn.Sequential(nn.Conv2d(IC,OC,K,S,P),\n",
        "                      nn.BatchNorm2d(OC),\n",
        "                      nn.LeakyReLU(),\n",
        "                      nn.Conv2d(OC,OC,K,S,P),\n",
        "                      nn.BatchNorm2d(OC),\n",
        "                      nn.LeakyReLU(),\n",
        "                      #SelfAttentionGen(OC),\n",
        "                      HierConv(OC,OC),\n",
        "                      nn.BatchNorm2d(OC),\n",
        "                      nn.LeakyReLU())\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return(self.layers(x))\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.conv1 = ConvBlock(3,8,3,1,1)\n",
        "    self.mp1 = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = ConvBlock(8,16,3,1,1)\n",
        "    self.mp2 = nn.MaxPool2d(2,2)\n",
        "    self.conv3 = ConvBlock(16,24,3,1,1)\n",
        "    self.mp3 = nn.MaxPool2d(2,2)\n",
        "    self.conv4 = ConvBlock(24,32,3,1,1)\n",
        "    self.mp4 = nn.MaxPool2d(2,2)\n",
        "    self.conv5 = ConvBlock(32,40,3,1,1)\n",
        "    self.up1 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "    self.conv6 = ConvBlock(40,32,3,1,1)\n",
        "    self.up2 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "    self.squeeze1 = ConvBlock(64,32,3,1,1)\n",
        "    self.conv7 = ConvBlock(32,24,3,1,1)\n",
        "    self.up3 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "    self.squeeze2 = ConvBlock(48,24,3,1,1)\n",
        "    self.conv8 = ConvBlock(24,16,3,1,1)\n",
        "    self.up4 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "    self.squeeze3 = ConvBlock(32,16,3,1,1)\n",
        "    self.conv9 = ConvBlock(16,8,3,1,1)\n",
        "    self.out = nn.Conv2d(8,3,3,1,1)\n",
        "\n",
        "  def forward(self,x):                                    # 3 * 256 * 256\n",
        "    c1=self.conv1(x)                                      # 8 * 256 * 256\n",
        "    cm1=self.mp1(c1)                                      # 8 * 128 * 128\n",
        "    c2=self.conv2(cm1)                                    # 16 * 128 * 128\n",
        "    cm2 =self.mp2(c2)                                     # 16 * 64 * 64\n",
        "    c3=self.conv3(cm2)                                    # 24 * 64 * 64\n",
        "    cm3 = self.mp3(c3)                                    # 24 * 32 * 32\n",
        "    c4=self.conv4(cm3)                                    # 32 * 32 * 32\n",
        "    cm4 = self.mp4(c4)                                    # 32 * 16 * 16\n",
        "    mid = self.conv5(cm4)                                 # 40 * 16 * 16\n",
        "    u1 = self.up1(mid)                                    # 40 * 32 * 32\n",
        "    uc1 = torch.cat((self.conv6(u1),c4), dim=1)           # 40 * 32 * 32\n",
        "    uc1s = self.squeeze1(uc1)                             # 32 * 32 * 32\n",
        "    u2 = self.up2(uc1s)                                   # 32 * 64 * 64\n",
        "    uc2 = torch.cat((self.conv7(u2), c3), dim=1)          # 32 * 64 * 64\n",
        "    uc2s = self.squeeze2(uc2)                             # 24 * 64 * 64\n",
        "    u3 = self.up3(uc2s)                                   # 24 * 128 * 128\n",
        "    uc3 = torch.cat((self.conv8(u3), c2), dim=1)          # 24 * 128 * 128\n",
        "    uc3s = self.squeeze3(uc3)                             # 16 * 128 * 128\n",
        "    u4 = self.up4(uc3s)                                   # 16 * 256 * 256\n",
        "    uc4 = self.conv9(u4)                                  # 8 * 256 * 256\n",
        "    ucfinal = self.out(uc4)                               # 3 * 256 * 256\n",
        "    return(torch.tanh(ucfinal))\n",
        "    #norm = ucfinal.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
        "    #out = (ucfinal/norm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionDisc(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(SelfAttentionDisc, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(1)\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        #print(2)\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
        "        #print(3)\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, height * width)\n",
        "        #print(4)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        #print(5)\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "        #print(6)\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, height * width)\n",
        "        #print(7)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        #print(8)\n",
        "        out = out.view(batch_size, channels, height, width)\n",
        "        #print(9)\n",
        "        out = self.gamma * out + x\n",
        "        #print(10)\n",
        "        return out"
      ],
      "metadata": {
        "id": "IljB6CTISV45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-y-HXkw2Gz-"
      },
      "outputs": [],
      "source": [
        "class DiscConvBlock(nn.Module):\n",
        "  def __init__(self,IC,OC,K,S,P):\n",
        "    super(DiscConvBlock , self).__init__()\n",
        "    self.layers = nn.Sequential(nn.Conv2d(IC,OC,K,S,P),\n",
        "                                nn.BatchNorm2d(OC),\n",
        "                                nn.ReLU())\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return(self.layers(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.conv1=DiscConvBlock(3,8,3,1,1)\n",
        "    #self.attention1 = SelfAttentionDisc(8)\n",
        "    self.mp1 = nn.MaxPool2d(2,2)\n",
        "    self.conv2=DiscConvBlock(8,16,3,1,1)\n",
        "    #self.attention2 = SelfAttentionDisc(16)\n",
        "    self.mp2 = nn.MaxPool2d(2,2)\n",
        "    self.conv3=DiscConvBlock(16,32,3,1,1)\n",
        "    #self.attention3 = SelfAttentionDisc(32)\n",
        "    self.mp3 = nn.MaxPool2d(2,2)\n",
        "    self.conv4=DiscConvBlock(32,64,3,1,1)\n",
        "    #self.attention4 = SelfAttentionDisc(64)\n",
        "    self.mp4 = nn.MaxPool2d(2,2)\n",
        "    self.conv5=DiscConvBlock(64,128,3,1,1)\n",
        "    self.attention5 = SelfAttentionDisc(128)\n",
        "\n",
        "    self.avgpl = nn.AvgPool2d(16,16)\n",
        "    self.linear1 = nn.Linear(128,64)\n",
        "    self.linear2 = nn.Linear(64,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    c1 = self.conv1(x)       # 3 * 256 * 256\n",
        "    #c1 = self.attention1(c1) # apply self-attention\n",
        "    cm1 = self.mp1(c1)       # 3 * 128 * 128\n",
        "    c2 = self.conv2(cm1)     # 8 * 128 * 128\n",
        "    #c2 = self.attention2(c2) # apply self-attention\n",
        "    cm2 = self.mp2(c2)       # 8 * 64 * 64\n",
        "    c3 = self.conv3(cm2)     # 16 * 64 * 64\n",
        "    #c3 = self.attention3(c3) # apply self-attention\n",
        "    cm3 = self.mp3(c3)       # 16 * 32 * 32\n",
        "    c4 = self.conv4(cm3)     # 32 * 32 * 32\n",
        "    #c4 = self.attention4(c4)\n",
        "    cm4 = self.mp4(c4)       # 32 * 16 * 16\n",
        "    c5 = self.conv5(cm4)     # 64 * 16 * 16\n",
        "    #c5 = self.attention5(c5)\n",
        "    avg = self.avgpl(c5).squeeze() # 128 * 1 * 1\n",
        "    l1 = self.linear1(avg)   # 64 * 1 * 1\n",
        "    l2 = self.linear2(l1)    # 1\n",
        "    return(l2)\n",
        "\n",
        "#isc = Discriminator().to(device)\n",
        "#inp = torch.randn(4,3,256,256).to(device)\n",
        "#out = disc(inp)\n",
        "#print(out.shape)\n",
        "#save_image(inp, \"inp.png\")\n",
        "#save_image(out, \"out.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sioql1HN2Jd-"
      },
      "outputs": [],
      "source": [
        "gen = Generator().to(device)\n",
        "disc = Discriminator().to(device)\n",
        "\n",
        "optGen = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optDisc = optim.Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Training Loop without WGAN\n",
        "lossWeight = 0.990\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  gen.train()\n",
        "  disc.train()\n",
        "  totalLoss = 0\n",
        "  epochLoss = 0\n",
        "  perEpochLoss = 0\n",
        "  l = len(trainloader)\n",
        "  for i, (bI,sI) in enumerate(trainloader):\n",
        "    if torch.cuda.is_available():\n",
        "      bI, sI = bI.to(device), sI.to(device)\n",
        "\n",
        "      # Training Discriminator  \n",
        "      optDisc.zero_grad()  \n",
        "      discOutReal = disc(sI)\n",
        "      genOutD = gen(bI)  \n",
        "      discOutFake = disc(genOutD)\n",
        "      lossReal = critDisc(discOutReal, torch.ones_like(discOutReal).to(device))\n",
        "      lossFake = critDisc(discOutFake, torch.zeros_like(discOutFake).to(device))\n",
        "      lossDisc = (lossReal+lossFake)/2.0\n",
        "      lossDisc.backward(retain_graph=True)\n",
        "      optDisc.step()\n",
        "\n",
        "      # Training Generator\n",
        "      optGen.zero_grad()\n",
        "      genOut = gen(bI)    \n",
        "      discOut = disc(genOut)\n",
        "      lossDiscGen = critDisc(discOut, torch.ones_like(discOut).to(device))\n",
        "      lossGen = (1-lossWeight)*lossDiscGen + lossWeight*critGen(genOut, sI)\n",
        " \n",
        "      lossGen.backward()\n",
        "      optGen.step()\n",
        "\n",
        "      totalLoss += (lossDisc.detach() + lossGen.detach())\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      if(i==l-2):\n",
        "\n",
        "        save_image(bI, \"/content/drive/MyDrive/finalTraining/40xImageTrain/%03d_blurimage.png\"%epoch, nrow=8)\n",
        "        save_image(sI, \"/content/drive/MyDrive/finalTraining/40xImageTrain/%03d_sharpimage.png\"%epoch, nrow=8)\n",
        "        save_image(genOutD, \"/content/drive/MyDrive/finalTraining/40xImageTrain/%03d_genimage.png\"%epoch, nrow=8)   \n",
        "\n",
        "  sampleLoss = totalLoss/(len(data))\n",
        "  print(f\"Epoch[{epoch+1}/{NUM_EPOCHS}] \\\n",
        "                Loss D: {lossDisc:.4f}, \\\n",
        "                loss G: {lossGen:.4f}, \\\n",
        "                loss per sample: {sampleLoss:.4f}\\n\")"
      ],
      "metadata": {
        "id": "RX4dwJFcPyHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st attempt\n",
        "# New Modifications to training loop with Gradient Penalty(WGAN-GP)\n",
        "\n",
        "gen = Generator().to(device)\n",
        "disc = Discriminator().to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "n_critic = 5\n",
        "lambda_gp = 10.0\n",
        "l = len(trainloader)\n",
        "\n",
        "# Defining the optimizers for the generator and discriminator\n",
        "gen_optimizer = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "disc_optimizer = optim.Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Defining the WGAN-GP loss function\n",
        "def wgan_gp_loss(scores_real, scores_fake, real_images, fake_images, disc):\n",
        "    batch_size = real_images.size(0)\n",
        "    epsilon = torch.rand(batch_size, 1, 1, 1).to(device)\n",
        "    interpolated_images = (epsilon * real_images + (1 - epsilon) * fake_images).requires_grad_(True)\n",
        "    interpolated_scores = disc(interpolated_images)\n",
        "    grad_outputs = torch.ones_like(interpolated_scores).to(device)\n",
        "    gradients = torch.autograd.grad(outputs=interpolated_scores, inputs=interpolated_images, grad_outputs=grad_outputs,\n",
        "                                    create_graph=True, retain_graph=True)[0]\n",
        "    gradient_penalty = ((gradients.view(batch_size, -1).norm(2, dim=1) - 1) ** 2).mean()\n",
        "    wgan_loss = torch.mean(scores_fake) - torch.mean(scores_real)\n",
        "    wgan_gp_loss = wgan_loss + lambda_gp * gradient_penalty\n",
        "    return wgan_gp_loss\n",
        "\n",
        "# Training the GAN model with WGAN-GP\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, (blur_images, sharp_images) in enumerate(trainloader):\n",
        "\n",
        "        # Training the discriminator\n",
        "        disc_optimizer.zero_grad()\n",
        "        real_scores = disc(sharp_images.to(device))\n",
        "        fake_images = gen(blur_images.to(device))\n",
        "        fake_scores = disc(fake_images.detach())\n",
        "        disc_loss = wgan_gp_loss(real_scores, fake_scores, sharp_images.to(device), fake_images.detach(), disc)\n",
        "        disc_loss.backward()\n",
        "        disc_optimizer.step()\n",
        "\n",
        "        # Training the generator\n",
        "        if i % n_critic == 0:\n",
        "            gen_optimizer.zero_grad()\n",
        "            fake_scores = disc(fake_images)\n",
        "            gen_loss = -torch.mean(fake_scores)\n",
        "            gen_loss.backward()\n",
        "            gen_optimizer.step()\n",
        "\n",
        "        if(i==l-2):\n",
        "                save_image(blur_images, \"/content/drive/MyDrive/finalTraining/3.10xGPmodified/%03d_blurimage.png\"%epoch, nrow=8)\n",
        "                save_image(sharp_images, \"/content/drive/MyDrive/finalTraining/3.10xGPmodified/%03d_sharpimage.png\"%epoch, nrow=8)\n",
        "                save_image(fake_images, \"/content/drive/MyDrive/finalTraining/3.10xGPmodified/%03d_genimage.png\"%epoch, nrow=8)\n",
        "\n",
        "    # Printing the epoch losses\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Gen Loss: {gen_loss.item():.4f}, Disc Loss: {disc_loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cx2cqafYuor",
        "outputId": "d13d791f-5a5e-4a7c-eb6a-19c2eb32fe1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30: Gen Loss: -0.8049, Disc Loss: -0.7329\n",
            "Epoch 2/30: Gen Loss: 0.1747, Disc Loss: -2.3318\n",
            "Epoch 3/30: Gen Loss: 2.4879, Disc Loss: -7.9104\n",
            "Epoch 4/30: Gen Loss: 6.5007, Disc Loss: -17.5248\n",
            "Epoch 5/30: Gen Loss: 7.4579, Disc Loss: -35.0493\n",
            "Epoch 6/30: Gen Loss: 10.4767, Disc Loss: -20.2416\n",
            "Epoch 7/30: Gen Loss: 7.4447, Disc Loss: -28.5406\n",
            "Epoch 8/30: Gen Loss: 2.2457, Disc Loss: -12.5903\n",
            "Epoch 9/30: Gen Loss: -0.9436, Disc Loss: -17.4500\n",
            "Epoch 10/30: Gen Loss: 2.3176, Disc Loss: -18.2446\n",
            "Epoch 11/30: Gen Loss: 1.0535, Disc Loss: -6.6858\n",
            "Epoch 12/30: Gen Loss: -5.4801, Disc Loss: -6.4486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd attempt with WGAN\n",
        "# Hyperparameters\n",
        "lambda_gp = 10.0\n",
        "num_critic_iter = 5\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    gen.train()\n",
        "    disc.train()\n",
        "    totalLoss = 0\n",
        "    epochLoss = 0\n",
        "    perEpochLoss = 0\n",
        "    l = len(trainloader)\n",
        "\n",
        "    for i, (bI,sI) in enumerate(trainloader):\n",
        "        if torch.cuda.is_available():\n",
        "            bI, sI = bI.to(device), sI.to(device)\n",
        "\n",
        "            # Training Discriminator\n",
        "            for _ in range(num_critic_iter):\n",
        "                optDisc.zero_grad()\n",
        "                with torch.no_grad():\n",
        "                    genOutD = gen(bI)\n",
        "                discOutReal = disc(sI)\n",
        "                discOutFake = disc(genOutD)\n",
        "\n",
        "                # Computing Wasserstein distance\n",
        "                wass_dist = discOutReal.mean() - discOutFake.mean()\n",
        "\n",
        "                # Computing gradient penalty\n",
        "                alpha = torch.rand(sI.size(0), 1, 1, 1).to(device)\n",
        "                interpolated = (alpha * sI + (1 - alpha) * genOutD).requires_grad_(True)\n",
        "                discOutInterpolated = disc(interpolated)\n",
        "                grad = torch.autograd.grad(outputs=discOutInterpolated, inputs=interpolated,\n",
        "                                           grad_outputs=torch.ones_like(discOutInterpolated),\n",
        "                                           create_graph=True, retain_graph=True)[0]\n",
        "                grad = grad.view(grad.size(0), -1)\n",
        "                grad_norm = grad.norm(2, dim=1)\n",
        "                gradient_penalty = lambda_gp * ((grad_norm - 1) ** 2).mean()\n",
        "\n",
        "                # Computing the total loss\n",
        "                lossDisc = -wass_dist + gradient_penalty\n",
        "                lossDisc.backward()\n",
        "                optDisc.step()\n",
        "\n",
        "            # Training Generator\n",
        "            optGen.zero_grad()\n",
        "            genOut = gen(bI)\n",
        "            discOut = disc(genOut)\n",
        "            lossGen = -discOut.mean()\n",
        "            lossGen.backward()\n",
        "            optGen.step()\n",
        "\n",
        "            totalLoss += (lossDisc.detach() + lossGen.detach())\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if(i==l-2):\n",
        "                save_image(bI, \"/content/drive/MyDrive/finalTraining/2.10xGP/%03d_blurimage.png\"%epoch, nrow=8)\n",
        "                save_image(sI, \"/content/drive/MyDrive/finalTraining/2.10xGP/%03d_sharpimage.png\"%epoch, nrow=8)\n",
        "                save_image(genOutD, \"/content/drive/MyDrive/finalTraining/2.10xGP/%03d_genimage.png\"%epoch, nrow=8)\n",
        "\n",
        "    sampleLoss = totalLoss/(len(data))\n",
        "    print(f\"Epoch[{epoch+1}/{NUM_EPOCHS}] \\\n",
        "                Loss D: {lossDisc:.4f}, \\\n",
        "                loss G: {lossGen:.4f}, \\\n",
        "                loss per sample: {sampleLoss:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ignozOOnw3S8",
        "outputId": "88f7ee75-a4e4-484a-a37f-1ca57a6ddc9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1/30]                 Loss D: -37.2030,                 loss G: 22.9717,                 loss per sample: -0.9349\n",
            "\n",
            "Epoch[2/30]                 Loss D: -30.1968,                 loss G: 22.0159,                 loss per sample: -4.3347\n",
            "\n",
            "Epoch[3/30]                 Loss D: -51.2278,                 loss G: 18.2418,                 loss per sample: -2.0244\n",
            "\n",
            "Epoch[4/30]                 Loss D: -37.0505,                 loss G: 22.3241,                 loss per sample: 1.6311\n",
            "\n",
            "Epoch[5/30]                 Loss D: -3.1104,                 loss G: 5.1461,                 loss per sample: 50.8786\n",
            "\n",
            "Epoch[6/30]                 Loss D: -2.2347,                 loss G: 6.8250,                 loss per sample: 2.3301\n",
            "\n",
            "Epoch[7/30]                 Loss D: -4.6622,                 loss G: 6.2931,                 loss per sample: 2.7335\n",
            "\n",
            "Epoch[8/30]                 Loss D: -10.5655,                 loss G: 8.6185,                 loss per sample: 0.2081\n",
            "\n",
            "Epoch[9/30]                 Loss D: -11.9473,                 loss G: 6.8863,                 loss per sample: -0.5705\n",
            "\n",
            "Epoch[10/30]                 Loss D: -12.7059,                 loss G: 2.8895,                 loss per sample: -2.1798\n",
            "\n",
            "Epoch[11/30]                 Loss D: -15.6960,                 loss G: 7.2805,                 loss per sample: -1.8581\n",
            "\n",
            "Epoch[12/30]                 Loss D: -6.8729,                 loss G: 7.7800,                 loss per sample: -2.2727\n",
            "\n",
            "Epoch[13/30]                 Loss D: -17.0567,                 loss G: 7.5848,                 loss per sample: -2.3125\n",
            "\n",
            "Epoch[14/30]                 Loss D: -5.2334,                 loss G: 24.0260,                 loss per sample: -1.7083\n",
            "\n",
            "Epoch[15/30]                 Loss D: -5.9911,                 loss G: 34.8602,                 loss per sample: 9.6344\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd attempt\n",
        "#TRAINING LOOP\n",
        "\n",
        "lossWeight = 0.990\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  gen.train()\n",
        "  disc.train()\n",
        "  totalLoss = 0\n",
        "  epochLoss = 0\n",
        "  perEpochLoss = 0\n",
        "  l = len(trainloader)\n",
        "  for i, (bI,sI) in enumerate(trainloader):\n",
        "    if torch.cuda.is_available():\n",
        "      bI, sI = bI.to(device), sI.to(device)\n",
        "\n",
        "      # Training Discriminator  \n",
        "      optDisc.zero_grad()  \n",
        "      discOutReal = disc(sI)\n",
        "      genOutD = gen(bI)  \n",
        "      discOutFake = disc(genOutD)\n",
        "      lossReal = discOutReal.mean()\n",
        "      lossFake = discOutFake.mean()\n",
        "      gradient_penalty = calc_gradient_penalty(disc, sI, genOutD)\n",
        "      lossDisc = lossFake - lossReal + 10 * gradient_penalty\n",
        "      lossDisc.backward(retain_graph=True)\n",
        "      optDisc.step()\n",
        "\n",
        "      # Training Generator\n",
        "      optGen.zero_grad()\n",
        "      genOut = gen(bI)    \n",
        "      discOut = disc(genOut)\n",
        "      lossDiscGen = discOut.mean()\n",
        "      lossGen = (1-lossWeight)*lossDiscGen + lossWeight*F.mse_loss(genOut, sI)\n",
        " \n",
        "      lossGen.backward()\n",
        "      optGen.step()\n",
        "\n",
        "      totalLoss += (lossDisc.detach() + lossGen.detach())\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      if(i==l-2):\n",
        "\n",
        "        save_image(bI, \"/content/drive/MyDrive/finalTraining/10xImageGradientPenalty/%03d_blurimage.png\"%epoch, nrow=8)\n",
        "        save_image(sI, \"/content/drive/MyDrive/finalTraining/10xImageGradientPenalty/%03d_sharpimage.png\"%epoch, nrow=8)\n",
        "        save_image(genOutD, \"/content/drive/MyDrive/finalTraining/10xImageGradientPenalty/%03d_genimage.png\"%epoch, nrow=8)   \n",
        "\n",
        "  sampleLoss = totalLoss/(len(data))\n",
        "  print(f\"Epoch[{epoch+1}/{NUM_EPOCHS}] \\\n",
        "                Loss D: {lossDisc:.4f}, \\\n",
        "                loss G: {lossGen:.4f}, \\\n",
        "                loss per sample: {sampleLoss:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "YIhQ0JPpW060",
        "outputId": "443d89c0-af43-4475-aa3a-95d0361d5192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1/30]                 Loss D: -0.3038,                 loss G: 0.2278,                 loss per sample: 3.5087\n",
            "\n",
            "Epoch[2/30]                 Loss D: -35.8170,                 loss G: -0.1808,                 loss per sample: -8.4459\n",
            "\n",
            "Epoch[3/30]                 Loss D: -97.5406,                 loss G: -0.5188,                 loss per sample: -31.1443\n",
            "\n",
            "Epoch[4/30]                 Loss D: -192.5621,                 loss G: -0.9930,                 loss per sample: -68.1617\n",
            "\n",
            "Epoch[5/30]                 Loss D: -314.9494,                 loss G: -1.6549,                 loss per sample: -118.1484\n",
            "\n",
            "Epoch[6/30]                 Loss D: -460.7984,                 loss G: -2.4020,                 loss per sample: -185.1122\n",
            "\n",
            "Epoch[7/30]                 Loss D: -617.3685,                 loss G: -3.1913,                 loss per sample: -263.6455\n",
            "\n",
            "Epoch[8/30]                 Loss D: -839.8500,                 loss G: -4.1734,                 loss per sample: -353.6237\n",
            "\n",
            "Epoch[9/30]                 Loss D: -1048.6648,                 loss G: -5.4979,                 loss per sample: -468.4184\n",
            "\n",
            "Epoch[10/30]                 Loss D: -1321.3108,                 loss G: -6.7870,                 loss per sample: -586.9618\n",
            "\n",
            "Epoch[11/30]                 Loss D: -1690.4210,                 loss G: -8.3987,                 loss per sample: -743.5576\n",
            "\n",
            "Epoch[12/30]                 Loss D: -2041.3394,                 loss G: -10.1121,                 loss per sample: -909.9400\n",
            "\n",
            "Epoch[13/30]                 Loss D: -2363.3745,                 loss G: -11.5411,                 loss per sample: -1086.1116\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ecbcb0615edf>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mlossGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlossWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlossDiscGen\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlossWeight\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenOut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mlossGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0moptGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gfVW3JA2MFN"
      },
      "outputs": [],
      "source": [
        "# 4th attempt \n",
        "#TRAINING LOOP\n",
        "clip_value = 0.01\n",
        "n_critic = 5\n",
        "lossWeight = 0.990\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  gen.train()\n",
        "  disc.train()\n",
        "  totalLoss = 0\n",
        "  epochLoss = 0\n",
        "  perEpochLoss = 0\n",
        "  l = len(trainloader)\n",
        "  for i, (bI,sI) in enumerate(trainloader):\n",
        "    if torch.cuda.is_available():\n",
        "      bI, sI = bI.to(device), sI.to(device)\n",
        "\n",
        "      # Training Discriminator\n",
        "      for n in range(n_critic): # discriminator will be trained n_critic times before updating Generator's weights\n",
        "          optDisc.zero_grad()  \n",
        "          discOutReal = disc(sI)\n",
        "          genOutD = gen(bI)  \n",
        "          discOutFake = disc(genOutD.detach())\n",
        "          lossDisc = wasserstein_loss(discOutReal - discOutFake, torch.ones_like(discOutReal).to(device))\n",
        "          lossDisc.backward()\n",
        "          optDisc.step()\n",
        "\n",
        "          # Clip weights\n",
        "          for p in disc.parameters():\n",
        "              p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "      # Training Generator\n",
        "      optGen.zero_grad()\n",
        "      genOut = gen(bI)    \n",
        "      discOut = disc(genOut)\n",
        "      lossGen = -wasserstein_loss(discOut, torch.ones_like(discOut).to(device))\n",
        "      lossGen.backward()\n",
        "      optGen.step()\n",
        "\n",
        "      totalLoss += (lossDisc.detach() + lossGen.detach())\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      if(i==l-2):\n",
        "\n",
        "        save_image(bI, \"/content/drive/MyDrive/finalTraining/10xHierConv()Image/%03d_blurimage.png\"%epoch, nrow=8)\n",
        "        save_image(sI, \"/content/drive/MyDrive/finalTraining/10xHierConv()Image/%03d_sharpimage.png\"%epoch, nrow=8)\n",
        "        save_image(genOutD, \"/content/drive/MyDrive/finalTraining/10xHierConv()Image/%03d_genimage.png\"%epoch, nrow=8)   \n",
        "\n",
        "  sampleLoss = totalLoss/(len(data))\n",
        "  print(f\"Epoch[{epoch+1}/{NUM_EPOCHS}] \\\n",
        "                Loss D: {lossDisc:.4f}, \\\n",
        "                loss G: {lossGen:.4f}, \\\n",
        "                loss per sample: {sampleLoss:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(disc,(3,256,256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWxX4OyBahuX",
        "outputId": "fd2e73c4-821c-430c-a718-3a750e9d3d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 256, 256]             224\n",
            "       BatchNorm2d-2          [-1, 8, 256, 256]              16\n",
            "              ReLU-3          [-1, 8, 256, 256]               0\n",
            "     DiscConvBlock-4          [-1, 8, 256, 256]               0\n",
            "         MaxPool2d-5          [-1, 8, 128, 128]               0\n",
            "            Conv2d-6         [-1, 16, 128, 128]           1,168\n",
            "       BatchNorm2d-7         [-1, 16, 128, 128]              32\n",
            "              ReLU-8         [-1, 16, 128, 128]               0\n",
            "     DiscConvBlock-9         [-1, 16, 128, 128]               0\n",
            "        MaxPool2d-10           [-1, 16, 64, 64]               0\n",
            "           Conv2d-11           [-1, 32, 64, 64]           4,640\n",
            "      BatchNorm2d-12           [-1, 32, 64, 64]              64\n",
            "             ReLU-13           [-1, 32, 64, 64]               0\n",
            "    DiscConvBlock-14           [-1, 32, 64, 64]               0\n",
            "        MaxPool2d-15           [-1, 32, 32, 32]               0\n",
            "           Conv2d-16           [-1, 64, 32, 32]          18,496\n",
            "      BatchNorm2d-17           [-1, 64, 32, 32]             128\n",
            "             ReLU-18           [-1, 64, 32, 32]               0\n",
            "    DiscConvBlock-19           [-1, 64, 32, 32]               0\n",
            "        MaxPool2d-20           [-1, 64, 16, 16]               0\n",
            "           Conv2d-21          [-1, 128, 16, 16]          73,856\n",
            "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
            "             ReLU-23          [-1, 128, 16, 16]               0\n",
            "    DiscConvBlock-24          [-1, 128, 16, 16]               0\n",
            "           Conv2d-25           [-1, 16, 16, 16]           2,064\n",
            "           Conv2d-26           [-1, 16, 16, 16]           2,064\n",
            "           Conv2d-27          [-1, 128, 16, 16]          16,512\n",
            "SelfAttentionDisc-28          [-1, 128, 16, 16]               0\n",
            "        AvgPool2d-29            [-1, 128, 1, 1]               0\n",
            "           Linear-30                   [-1, 64]           8,256\n",
            "           Linear-31                    [-1, 1]              65\n",
            "================================================================\n",
            "Total params: 127,841\n",
            "Trainable params: 127,841\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.75\n",
            "Forward/backward pass size (MB): 33.44\n",
            "Params size (MB): 0.49\n",
            "Estimated Total Size (MB): 34.68\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(gen,(3,256,256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "RDsFzP3-2rdl",
        "outputId": "9a4aa8ee-98bb-42a0-a2de-b45947cc500a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-04bc5319c8e3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5f744759794d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                    \u001b[0;31m# 3 * 256 * 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;31m# 8 * 256 * 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mcm1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;31m# 8 * 128 * 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm1\u001b[0m\u001b[0;34m)\u001b[0m                                    \u001b[0;31m# 16 * 128 * 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             for hook_id, hook in (\n",
            "\u001b[0;32m<ipython-input-9-5f744759794d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             for hook_id, hook in (\n",
            "\u001b[0;32m<ipython-input-76-b5ffb0585201>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mproj_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 GiB (GPU 0; 14.75 GiB total capacity; 179.81 MiB already allocated; 13.49 GiB free; 222.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for params in disc.parameters():\n",
        "  print(np.prod(np.array(params.shape)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzyDkq-_dF29",
        "outputId": "63ce1a79-6c62-4ec2-d50a-07ff63371388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "216\n",
            "8\n",
            "8\n",
            "8\n",
            "1\n",
            "8\n",
            "1\n",
            "8\n",
            "1\n",
            "64\n",
            "8\n",
            "1152\n",
            "16\n",
            "16\n",
            "16\n",
            "1\n",
            "32\n",
            "2\n",
            "32\n",
            "2\n",
            "256\n",
            "16\n",
            "4608\n",
            "32\n",
            "32\n",
            "32\n",
            "1\n",
            "128\n",
            "4\n",
            "128\n",
            "4\n",
            "1024\n",
            "32\n",
            "18432\n",
            "64\n",
            "64\n",
            "64\n",
            "1\n",
            "512\n",
            "8\n",
            "512\n",
            "8\n",
            "4096\n",
            "64\n",
            "73728\n",
            "128\n",
            "128\n",
            "128\n",
            "1\n",
            "2048\n",
            "16\n",
            "2048\n",
            "16\n",
            "16384\n",
            "128\n",
            "8192\n",
            "64\n",
            "64\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genHierConv = Generator().to(device)\n",
        "mo = torch.load('/content/drive/MyDrive/finalTraining/HierConv()Model.pt')\n",
        "model = genHierConv.load_state_dict(mo)"
      ],
      "metadata": {
        "id": "qVJKRKgVZ9l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qayDEBONEslq"
      },
      "outputs": [],
      "source": [
        "torch.save(gen.cpu().state_dict(),\"/content/drive/MyDrive/finalTraining/HierConv()Model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op_UDgyZkDjM"
      },
      "outputs": [],
      "source": [
        "gen1 = Generator().to(device)\n",
        "mo = torch.load('//content/drive/MyDrive/GATE 2021/10xSavedModel.pt')\n",
        "model = gen1.load_state_dict(mo)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(disc)\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hju0mWYZfcoX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}